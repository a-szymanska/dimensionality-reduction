The KNN algorithm in a basic implementation has a time complexity of \( \bigO(m \cdot N) \), where \( m \) is the size of the dataset, \( N \) is the number of dimensions of the data-space. Reducing the dimensionality is therefore a straightforward optimisation leading to improvement in both memory and time complexity by reducing the size of the vectors and making comparisons faster. Although in sequential KNN procedure, the complexity is only improved by a certain constant, the reduction is much more relevant for advanced search methods.

Many of the currently adopted KNN search techniques are based on constructing a multidimensional data structure to perform clustering on the data \cite{weber1998knn}. A standard approach used among others in K-D-B-tree, quadtree, R-tree or SR-tree methods is to partition the data space and to search only in the relevant classes \cite{weber1998knn}. Such approaches reduce the number of necessary comparisons and are fundamental to faster KNN queries. However, the performance of this approach deteriorates rapidly with an increasing number of dimensions, as drastically as this phenomenon occurs in the literature as a ``dimensional curse'' \cite{weber1998knn}. Some frameworks, such as R-tree or SR-tree, attempt to deal with this problem by rejecting the assumption of uniform data distribution in space and restricting the search to the most significant dimensions \cite{katayama1997srtree}. Nevertheless, as demonstrated by Weber et al., for each partition-based algorithm, there is a number of dimensions \( N \) beyond which it converges to time complexity \( \bigO(N) \), thus a linear search of the collection is equally or more efficient \cite{weber1998knn}. In practice the boundary value is \( N \approx 600 \) \cite{weber1998knn}.

When the partitions are performed on every dimension, as in the case of K-D-tree and quadtree, the number of possible partitions is of order \( 2^N \). For \( N = 1024 \), as in the case of ECFP vectors, the number of classes is thus close to \( 10^{308} \), and such structures are not applicable for large \( N \). Moreover, in sets containing about a million vectors, the number significantly exceeds the size of the dataset, making most partition classes empty, especially for sparse representations such as ECFP. Far worse, despite a sparsely populated index structure, some classes may contain several points that need to be searched sequentially. Real datasets, such as random collections of chemical compounds, often contain clusters of similar objects \cite{berchtold2002xtree}. The compounds in one cluster may not even be distinguishable by ECFP fingerprints. Some dimensions in the data space may also be meaningless because, for example, the bit corresponding to -CH3 or -CH2- groups is present in the vast majority of fingerprints.

Dimensionality reduction is a versatile partial solution to the above problems. It does not solve them entirely, as sometimes the size of the resulting data space is still too big, nevertheless, it is advised for real-life cases whenever possible \cite{berchtold2002xtree}. By transforming high-dimensional data with methods such as Johnson-Lindenstrauss or MinHash reductions, it is possible to decrease the number of dimensions from \( N \) to the order of \( \log m \), thus making the number of partitions comparable to the size of the dataset. Consequently, in expectation, the data space is filled more evenly, and the index structure is less sparse. Thereby, reducing the number of dimensions may not only improve the complexity for sequential KNN-search but also enable the use of advanced index structures to perform queries in \( \bigO(m \cdot \log N) \) time complexity.